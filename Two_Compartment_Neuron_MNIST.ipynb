{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Two-compartment spiking neurons on MNIST\n",
        "\n",
        "This notebook explores how **two-compartment spiking neurons** behave on a real vision task: handwritten digit classification with **MNIST**.\n",
        "\n",
        "Instead of treating a neuron as a single lumped unit, we separate its inputs into:\n",
        "\n",
        "- **Basal compartment** – main feedforward input (image pixels)\n",
        "- **Apical compartment** – contextual or auxiliary input (e.g., blurred image, teacher signal, or other top-down information)\n",
        "\n",
        "Both compartments evolve over time using a **Leaky Integrate-and-Fire (LIF)** model implemented with `snnTorch`, and the network is trained with **surrogate gradient backpropagation**.\n",
        "\n",
        "---\n",
        "\n",
        "## What this notebook does\n",
        "\n",
        "1. **Load and preprocess MNIST**\n",
        "   - Standard train / test split\n",
        "   - Images are scaled and fed into a spiking network over multiple time steps\n",
        "\n",
        "2. **Define a two-compartment LIF neuron**\n",
        "   - Basal and apical inputs are processed separately\n",
        "   - Their voltages are combined at the soma to decide when to spike\n",
        "   - Spikes are accumulated over time and passed to a readout layer for classification\n",
        "\n",
        "3. **Build a two-compartment SNN for MNIST**\n",
        "   - Input: MNIST images (`28×28`)\n",
        "   - Hidden layer: spiking neurons with basal + apical compartments\n",
        "   - Output: 10-way classification based on spike counts\n",
        "\n",
        "4. **Context experiments (apical as “extra information”)**\n",
        "   - Apical input receives a **different view** of the image (for example, a blurred / low-resolution version)\n",
        "   - The network is trained end-to-end to see whether the apical compartment actually helps, or if it gets ignored\n",
        "\n",
        "5. **Ablation tests**\n",
        "   - Evaluate the trained model under:\n",
        "     - Normal conditions (basal + apical)\n",
        "     - **No apical** (apical channel zeroed out)\n",
        "     - **No basal** (basal channel zeroed out)\n",
        "   - This shows whether the network truly relies on each compartment, and whether they play different roles.\n",
        "\n",
        "---\n",
        "\n",
        "The goal is not just to “get good accuracy,” but to ask:\n",
        "\n",
        "> When we give artificial neurons a more biologically inspired structure (separate basal and apical compartments),  \n",
        "> do they end up using those compartments in meaningful ways?\n"
      ],
      "metadata": {
        "id": "MzsN3SMO_gzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "RgqmcNdMho9P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to ensure reproducibility\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)\n"
      ],
      "metadata": {
        "id": "pQOhjUjmx_KN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "\n",
        "spike_grad = surrogate.fast_sigmoid()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcHqr86XiMZ0",
        "outputId": "ddc8795d-a61a-4e31-ebac-4b66a77b4e4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snntorch\n",
            "Successfully installed snntorch-0.9.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two-compartment LIF neuron\n",
        "\n",
        "Here we define a **two-compartment Leaky Integrate-and-Fire (LIF) neuron**.\n",
        "\n",
        "Instead of one lumped input, each neuron has:\n",
        "\n",
        "- A **basal compartment** that receives one input vector (e.g., pixels or features).\n",
        "- An **apical compartment** that receives a *separate* input vector (e.g., context, blurred image, or teacher signal).\n",
        "\n",
        "For each time step:\n",
        "\n",
        "1. Basal input is projected through a weight matrix `W_basal` and added to a **basal voltage** `V_b`.\n",
        "2. Apical input is projected through `W_apical` and added to an **apical voltage** `V_a`.\n",
        "3. Both voltages **leak** over time (they decay if there is no input).\n",
        "4. The **soma voltage** is computed as:\n",
        "\n",
        "   > soma voltage = basal voltage + (apical_gain) × apical voltage\n",
        "\n",
        "5. If the soma voltage crosses a threshold, the neuron emits a spike at that time step and its voltage is reset.\n",
        "\n",
        "Because spikes are not differentiable, we use a **surrogate gradient** from `snnTorch` so we can still train this neuron with backpropagation.\n"
      ],
      "metadata": {
        "id": "95N9NUnmA10r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoCompartmentLIF(nn.Module):\n",
        "    def __init__(self, in_basal, in_apical, hidden_size,\n",
        "                 tau_b=20.0, tau_a=20.0,\n",
        "                 v_th=1.0, apical_gain=0.5, v_reset=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_basal  = nn.Linear(in_basal,  hidden_size, bias=False)\n",
        "        self.W_apical = nn.Linear(in_apical, hidden_size, bias=False)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau_b = tau_b\n",
        "        self.tau_a = tau_a\n",
        "        self.v_th = v_th\n",
        "        self.v_reset = v_reset\n",
        "        self.apical_gain = apical_gain\n",
        "\n",
        "        self.alpha_b = math.exp(-1.0 / self.tau_b)\n",
        "        self.alpha_a = math.exp(-1.0 / self.tau_a)\n",
        "\n",
        "    def forward(self, x_basal, x_apical):\n",
        "        \"\"\"\n",
        "        x_basal:  [T, B, in_basal]\n",
        "        x_apical: [T, B, in_apical]\n",
        "        Returns:\n",
        "            spikes: [T, B, hidden_size]\n",
        "            V_soma: [T, B, hidden_size]\n",
        "        \"\"\"\n",
        "        T, B, _ = x_basal.shape\n",
        "        device = x_basal.device\n",
        "\n",
        "        V_b = torch.zeros(B, self.hidden_size, device=device)\n",
        "        V_a = torch.zeros(B, self.hidden_size, device=device)\n",
        "\n",
        "        spikes = []\n",
        "        Vs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            xb_t = x_basal[t]   # [B, in_basal]\n",
        "            xa_t = x_apical[t]  # [B, in_apical]\n",
        "\n",
        "            I_b = self.W_basal(xb_t)   # [B,H]\n",
        "            I_a = self.W_apical(xa_t)  # [B,H]\n",
        "\n",
        "            V_b = self.alpha_b * V_b + I_b\n",
        "            V_a = self.alpha_a * V_a + I_a\n",
        "\n",
        "            V_soma = V_b + self.apical_gain * V_a\n",
        "\n",
        "            # surrogate spike (use your existing spike_grad)\n",
        "            s = spike_grad(V_soma - self.v_th)\n",
        "            s_hard = (V_soma >= self.v_th).float()\n",
        "\n",
        "            # reset on hard spikes\n",
        "            V_b = torch.where(s_hard > 0, torch.full_like(V_b, self.v_reset), V_b)\n",
        "            V_a = torch.where(s_hard > 0, torch.full_like(V_a, self.v_reset), V_a)\n",
        "\n",
        "            spikes.append(s)\n",
        "            Vs.append(V_soma)\n",
        "\n",
        "        spikes = torch.stack(spikes, dim=0)\n",
        "        Vs     = torch.stack(Vs, dim=0)\n",
        "        return spikes, Vs\n"
      ],
      "metadata": {
        "id": "-JLV2ElM6XRs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base two-compartment SNN for MNIST\n",
        "\n",
        "Next, we wrap the two-compartment neuron in a full network for MNIST.\n",
        "\n",
        "- Input: a batch of `B` images of shape `[B, 1, 28, 28]`.\n",
        "- We **flatten** each image into a vector of length 784.\n",
        "- We feed the same pixels into:\n",
        "  - the **basal** compartment, and\n",
        "  - the **apical** compartment (possibly with different masks or preprocessing in later experiments).\n",
        "\n",
        "The network runs for `T` time steps:\n",
        "\n",
        "- At each time step, it receives the same image as a rate-coded input.\n",
        "- The two-compartment neuron produces spikes over time.\n",
        "- We **sum spikes over time** for each hidden neuron.\n",
        "- A final linear layer maps spike counts to **10 logits** (one per digit).\n",
        "\n",
        "This gives us a standard spiking classifier, but with separated basal and apical inputs.\n"
      ],
      "metadata": {
        "id": "gCZle_elICOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoCompartmentMNISTNet(nn.Module):\n",
        "    def __init__(self, hidden_size=256, num_classes=10, num_steps=25):\n",
        "        super().__init__()\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self.tc_lif = TwoCompartmentLIF(\n",
        "            in_basal=28*28,\n",
        "            in_apical=28*28,\n",
        "            hidden_size=hidden_size,\n",
        "            v_th=1.0,\n",
        "            apical_gain=0.5    # now apical actually contributes\n",
        "        )\n",
        "        self.readout = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        images: [B, 1, 28, 28]\n",
        "        \"\"\"\n",
        "        B = images.size(0)\n",
        "        device = images.device\n",
        "\n",
        "        flat = images.view(B, -1)   # [B, 784]\n",
        "\n",
        "        # split pixels into basal vs apical according to mask\n",
        "        xb = flat * mask_basal      # [B, 784]\n",
        "        xa = flat * mask_apical     # [B, 784]\n",
        "\n",
        "        # repeat in time (rate code)\n",
        "        xb_seq = xb.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,784]\n",
        "        xa_seq = xa.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,784]\n",
        "\n",
        "        spikes, _ = self.tc_lif(xb_seq, xa_seq)  # [T,B,H]\n",
        "        spike_counts = spikes.sum(dim=0)         # [B,H]\n",
        "        logits = self.readout(spike_counts)      # [B,10]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "Ro5Cnxel6Yky"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "def train_snn(model, train_loader, test_loader, num_epochs=10, lr=1e-3, device=\"cuda\"):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_acc_hist = []\n",
        "    test_acc_hist = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)   # [B, 1, 28, 28]\n",
        "            labels = labels.to(device)   # [B]\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)       # [B, 10]\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_acc = total_correct / total_samples\n",
        "        train_loss = total_loss / total_samples\n",
        "        train_acc_hist.append(train_acc)\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                logits = model(images)\n",
        "                preds = logits.argmax(dim=1)\n",
        "\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "        test_acc = total_correct / total_samples\n",
        "        test_acc_hist.append(test_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:2d} | \"\n",
        "            f\"train loss {train_loss:.4f} | \"\n",
        "            f\"train acc {train_acc:.4f} | \"\n",
        "            f\"test acc {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "    return train_acc_hist, test_acc_hist\n"
      ],
      "metadata": {
        "id": "1xL-k2Ek6MWp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting MNIST pixels into basal vs apical inputs\n",
        "\n",
        "Getting the MNIST Dataset. Then, to give the two-compartment neuron two different input streams, we split each 28×28 MNIST image into **basal pixels** and **apical pixels**.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "- We flatten each image into a vector of length 784.\n",
        "- We randomly choose half of the indices to be **basal**.\n",
        "- The remaining pixels become **apical**.\n",
        "\n",
        "In code:\n",
        "\n",
        "- `mask_basal = 1` means “this pixel goes to the basal compartment”.\n",
        "- `mask_apical = 1` means “this pixel goes to the apical compartment”.\n",
        "\n",
        "This way, the overall information content is the same, but the neuron has to learn how to combine **two separate spatial streams** instead of seeing all pixels in a single compartment.\n"
      ],
      "metadata": {
        "id": "ZKNfYXQPMvFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                  # [0,1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# converting the MNIST"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Now72Vu48gBU",
        "outputId": "03397068-00cd-4872-d31f-8842c88536f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.64MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 130kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.59MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 28 * 28\n",
        "mask = torch.zeros(input_dim)\n",
        "perm = torch.randperm(input_dim)\n",
        "half = input_dim // 2\n",
        "mask[perm[:half]] = 1.0\n",
        "\n",
        "mask_basal = mask          # 1 = basal pixel\n",
        "mask_apical = 1.0 - mask   # 1 = apical pixel\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "mask_basal = mask_basal.to(device)\n",
        "mask_apical = mask_apical.to(device)\n"
      ],
      "metadata": {
        "id": "lE4jn8Y4Ig_i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample of what the data and the split looks like"
      ],
      "metadata": {
        "id": "Bm2d4x_oM7eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = datasets.MNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "img, label = mnist[0]\n",
        "img = img.squeeze().numpy()\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "mask_basal = np.random.rand(28, 28) > 0.5\n",
        "mask_apical = ~mask_basal\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10,4))\n",
        "\n",
        "axs[0].imshow(img, cmap='gray')\n",
        "axs[0].set_title(\"Original MNIST Digit\")\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(img * mask_basal, cmap='gray')\n",
        "axs[1].set_title(\"Basal Input\")\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(img * mask_apical, cmap='gray')\n",
        "axs[2].set_title(\"Apical Input\")\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5SaCAfF-M-In",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "032aa8f4-188e-43f8-c9a2-a9918ef52713"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.03MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.66MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Training and Ablations"
      ],
      "metadata": {
        "id": "XwPas4XQNaQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 28 * 28\n",
        "mask = torch.zeros(input_dim)\n",
        "perm = torch.randperm(input_dim)\n",
        "half = input_dim // 2\n",
        "mask[perm[:half]] = 1.0\n",
        "\n",
        "mask_basal = mask          # 1 = basal pixel\n",
        "mask_apical = 1.0 - mask   # 1 = apical pixel\n",
        "# device is already defined\n",
        "mask_basal = mask_basal.to(device)\n",
        "mask_apical = mask_apical.to(device)\n",
        "\n",
        "model = TwoCompartmentMNISTNet(hidden_size=128, num_classes=10)\n",
        "\n",
        "train_acc, test_acc = train_snn(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    num_epochs=4,\n",
        "    lr=1e-3,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA0pDw7rA_Hi",
        "outputId": "63f240da-6644-4a27-c6c5-70f69577fadc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0 | train loss 0.4046 | train acc 0.8881 | test acc 0.8912\n",
            "Epoch  1 | train loss 0.2490 | train acc 0.9280 | test acc 0.9417\n",
            "Epoch  2 | train loss 0.2335 | train acc 0.9340 | test acc 0.9428\n",
            "Epoch  3 | train loss 0.2299 | train acc 0.9338 | test acc 0.9182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, loader, ablate_apical=False, ablate_basal=False):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            B = images.size(0)\n",
        "            flat = images.view(B, -1)\n",
        "\n",
        "            # use same masks as in training\n",
        "            xb = flat * mask_basal\n",
        "            xa = flat * mask_apical\n",
        "\n",
        "            if ablate_apical:\n",
        "                xa = torch.zeros_like(xa)\n",
        "            if ablate_basal:\n",
        "                xb = torch.zeros_like(xb)\n",
        "\n",
        "            xb_seq = xb.unsqueeze(0).repeat(model.num_steps, 1, 1)\n",
        "            xa_seq = xa.unsqueeze(0).repeat(model.num_steps, 1, 1)\n",
        "\n",
        "            spikes, _ = model.tc_lif(xb_seq, xa_seq)\n",
        "            spike_counts = spikes.sum(dim=0)\n",
        "            logits = model.readout(spike_counts)\n",
        "\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "id": "w-1X89P_DKQG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_full     = eval_model(model, test_loader)\n",
        "acc_no_ap    = eval_model(model, test_loader, ablate_apical=True)\n",
        "acc_no_basal = eval_model(model, test_loader, ablate_basal=True)\n",
        "print(\"Test acc:\", acc_full)\n",
        "print(\"Test acc WITHOUT apical input:\", acc_no_ap)\n",
        "print(\"Test acc WITHOUT basal input:\", acc_no_basal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWJzYUXIDMXC",
        "outputId": "491efea9-503a-4964-e5b1-23b88534478b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.9181690705128205\n",
            "Test acc WITHOUT apical input: 0.9063501602564102\n",
            "Test acc WITHOUT basal input: 0.8118990384615384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the ablations above, the model relies on both inputs to get the full 0.93 accuracy since ablations cause a drop in accuracy."
      ],
      "metadata": {
        "id": "qAC62WS7NsY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment: blurred image as apical “context”\n",
        "\n",
        "In the next experiment, the two compartments get **different versions** of the same digit:\n",
        "\n",
        "- **Basal compartment**: full-resolution 28×28 MNIST image.\n",
        "- **Apical compartment**: a **blurred / low-resolution** version of the image.\n",
        "\n",
        "Implementation details:\n",
        "\n",
        "- We use `AvgPool2d` to downsample 28×28 → 7×7.\n",
        "- Then we upsample back to 28×28 with bilinear interpolation.\n",
        "- The result keeps the **global shape** but loses most of the fine detail.\n",
        "\n",
        "Idea:\n",
        "\n",
        "- Basal sees sharp edges and local detail.\n",
        "- Apical sees a coarse, global view that plays the role of **top-down context**.\n",
        "\n",
        "We then train the model and compare:\n",
        "\n",
        "- Test accuracy **with** blurred apical context.\n",
        "- Test accuracy when apical input is replaced by zeros.\n",
        "\n",
        "If removing the blurred apical input hurts performance, it suggests that the apical compartment is actively using this contextual signal. Below is an example of the data\n"
      ],
      "metadata": {
        "id": "BrljtfIiN4Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST image\n",
        "mnist = datasets.MNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "img, label = mnist[0]\n",
        "img = img.squeeze().numpy()\n",
        "\n",
        "# Basal = original\n",
        "basal = img\n",
        "\n",
        "# Apical = blurred version\n",
        "apical = gaussian_filter(img, sigma=2)   # adjust sigma to control blur strength\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Original\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Basal input\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(basal, cmap='gray')\n",
        "plt.title(\"Basal Input (sharp)\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Apical input\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(apical, cmap='gray')\n",
        "plt.title(\"Apical Input (blurred)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qoSYUQUoOHfF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "49917208-ae5c-494b-d0d7-d9380743be64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is an example of this data"
      ],
      "metadata": {
        "id": "Nf3tMwT0ONac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TwoCompartmentMNISTNetBlurContext(nn.Module):\n",
        "    def __init__(self, hidden_size=256, num_classes=10, num_steps=25, use_context=True):\n",
        "        super().__init__()\n",
        "        self.num_steps = num_steps\n",
        "        self.use_context = use_context\n",
        "\n",
        "        # two-compartment LIF: both basal and apical get 784-dim inputs\n",
        "        self.tc_lif = TwoCompartmentLIF(\n",
        "            in_basal=28*28,\n",
        "            in_apical=28*28,\n",
        "            hidden_size=hidden_size,\n",
        "            v_th=1.0,\n",
        "            apical_gain=0.5,   # apical actually contributes\n",
        "        )\n",
        "        self.readout = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # simple blur / low-res context: 4x4 average pooling -> 7x7, then upsample back\n",
        "        self.pool = nn.AvgPool2d(kernel_size=4, stride=4)  # 28x28 -> 7x7\n",
        "        self.upsample = nn.Upsample(size=(28, 28), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    def make_context(self, images):\n",
        "        \"\"\"\n",
        "        images: [B, 1, 28, 28]\n",
        "        returns ctx_images: [B, 1, 28, 28] blurred / low-res\n",
        "        \"\"\"\n",
        "        # downsample\n",
        "        x = self.pool(images)                # [B,1,7,7]\n",
        "        # upsample back to 28x28\n",
        "        x = self.upsample(x)                 # [B,1,28,28]\n",
        "        # (optional) normalize a bit\n",
        "        # x = (x - x.mean(dim=[2,3], keepdim=True)) / (x.std(dim=[2,3], keepdim=True) + 1e-5)\n",
        "        return x\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        images: [B, 1, 28, 28]\n",
        "        Basal: full-res image\n",
        "        Apical: blurred/low-res version of same image (if use_context=True)\n",
        "        \"\"\"\n",
        "        B = images.size(0)\n",
        "        device = images.device\n",
        "\n",
        "        # Basal: original pixels\n",
        "        xb = images.view(B, -1)  # [B, 784]\n",
        "\n",
        "        if self.use_context:\n",
        "            ctx_images = self.make_context(images)   # [B,1,28,28]\n",
        "            xa = ctx_images.view(B, -1)             # [B,784]\n",
        "        else:\n",
        "            # no apical context -> zeros\n",
        "            xa = torch.zeros_like(xb)               # [B,784]\n",
        "\n",
        "        # repeat in time (simple rate code)\n",
        "        xb_seq = xb.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,784]\n",
        "        xa_seq = xa.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,784]\n",
        "\n",
        "        spikes, _ = self.tc_lif(xb_seq, xa_seq)     # [T,B,H]\n",
        "        spike_counts = spikes.sum(dim=0)            # [B,H]\n",
        "        logits = self.readout(spike_counts)         # [B,10]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "fA2_jxeB0NlH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "\n",
        "def train_mnist(model, train_loader, test_loader, num_epochs=5, lr=1e-3, device=\"cpu\"):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_examples = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_examples\n",
        "        train_acc = total_correct / total_examples\n",
        "\n",
        "        # eval\n",
        "        model.eval()\n",
        "        total_correct, total_examples = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits = model(images)\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_examples += labels.size(0)\n",
        "        test_acc = total_correct / total_examples\n",
        "\n",
        "        print(f\"Epoch {epoch:2d} | train loss {train_loss:.4f} | \"\n",
        "              f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "MU8lj6t-492J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running training and evaluating"
      ],
      "metadata": {
        "id": "8n84-J_SOiJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_ctx = TwoCompartmentMNISTNetBlurContext(\n",
        "    hidden_size=256,\n",
        "    num_classes=10,\n",
        "    num_steps=25,\n",
        "    use_context=True,   # apical gets blurred input\n",
        ").to(device)\n",
        "\n",
        "train_mnist(model_ctx, train_loader, test_loader, num_epochs=5, lr=1e-3, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbQa9msb5FK5",
        "outputId": "17c8670d-d858-40bb-f9a4-39185ab0b8bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0 | train loss 0.5130 | train acc 0.8665 | test acc 0.8914\n",
            "Epoch  1 | train loss 0.3879 | train acc 0.8915 | test acc 0.8859\n",
            "Epoch  2 | train loss 0.3678 | train acc 0.8990 | test acc 0.9140\n",
            "Epoch  3 | train loss 0.3603 | train acc 0.9026 | test acc 0.9021\n",
            "Epoch  4 | train loss 0.3491 | train acc 0.9051 | test acc 0.9077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    total_correct, total_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits = model(images)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_examples += labels.size(0)\n",
        "    return total_correct / total_examples\n",
        "\n",
        "# After training:\n",
        "model_ctx.use_context = True\n",
        "acc_with_ctx = eval_model(model_ctx, test_loader, device=device)\n",
        "\n",
        "model_ctx.use_context = False\n",
        "acc_no_ctx = eval_model(model_ctx, test_loader, device=device)\n",
        "\n",
        "print(\"Test acc WITH blurred apical context :\", acc_with_ctx)\n",
        "print(\"Test acc WITHOUT apical context      :\", acc_no_ctx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IahKcrhu5IUQ",
        "outputId": "8f9c609e-533a-4768-8f23-327330b688b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc WITH blurred apical context : 0.9076522435897436\n",
            "Test acc WITHOUT apical context      : 0.9033453525641025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, the model does better without the context... interesting! Well, a negative result is still a result!"
      ],
      "metadata": {
        "id": "-h9oFHHsOqeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment: teacher network as apical feedback\n",
        "\n",
        "Here we test a different kind of “context”: a **top-down prediction** from a separate teacher network.\n",
        "\n",
        "1. We first train a standard ANN classifier (`TeacherNet`) on MNIST.\n",
        "   - It takes pixels as input.\n",
        "   - It outputs a 10-dimensional probability vector over digit classes.\n",
        "\n",
        "2. We then build a two-compartment spiking model where:\n",
        "   - **Basal** still receives the image pixels.\n",
        "   - **Apical** receives a transformed version of the teacher’s output\n",
        "     (for example, by repeating or projecting the 10-D probability vector).\n",
        "\n",
        "We compare:\n",
        "\n",
        "- Two-compartment SNN **with** teacher-based apical input.\n",
        "- The same architecture with apical input set to zero.\n",
        "\n",
        "This tests whether the network can make use of **high-level feedback** (a noisy but informative guess from the teacher) routed through the apical compartment.\n"
      ],
      "metadata": {
        "id": "U54AEAs0PC2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is training the teacher model"
      ],
      "metadata": {
        "id": "Vw40Ys-9Pd95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,1,28,28] OR [B,784]\n",
        "        if x.dim() == 4:\n",
        "            x = x.view(x.size(0), -1)  # [B,784]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)               # logits [B,10]\n",
        "        return x\n",
        "\n",
        "teacher = TeacherNet().to(device)\n",
        "\n",
        "optimizer_teacher = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_teacher(num_epochs=3):\n",
        "    teacher.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, total_correct, total_examples = 0.0, 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer_teacher.zero_grad()\n",
        "            logits = teacher(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer_teacher.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_examples\n",
        "        train_acc  = total_correct / total_examples\n",
        "\n",
        "        # quick test eval\n",
        "        teacher.eval()\n",
        "        tot_corr, tot_ex = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits = teacher(images)\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                tot_corr += (preds == labels).sum().item()\n",
        "                tot_ex   += labels.size(0)\n",
        "        test_acc = tot_corr / tot_ex\n",
        "        teacher.train()\n",
        "\n",
        "        print(f\"[Teacher] Epoch {epoch:2d} | train loss {train_loss:.4f} | \"\n",
        "              f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\")\n",
        "\n",
        "# Train the teacher (3–5 epochs is usually plenty for MNIST)\n",
        "train_teacher(num_epochs=3)\n",
        "\n",
        "# Freeze teacher\n",
        "teacher.eval()\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD3z6qYu7Hws",
        "outputId": "9d13cb0e-4b10-4b4f-ba08-7d099b9525b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Teacher] Epoch  0 | train loss 0.2604 | train acc 0.9232 | test acc 0.9581\n",
            "[Teacher] Epoch  1 | train loss 0.1137 | train acc 0.9657 | test acc 0.9714\n",
            "[Teacher] Epoch  2 | train loss 0.0775 | train acc 0.9756 | test acc 0.9735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the teacher does very well -- this is a standard ANN. ANNs already do pretty well on the MNIST task, but it's still worth exploring other architectures because they might have different advantages."
      ],
      "metadata": {
        "id": "jBdpU0P6PM5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoCompartmentMNISTNetWithTeacher(nn.Module):\n",
        "    def __init__(self, teacher, hidden_size=256, num_classes=10, num_steps=25):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        # Two-compartment LIF: basal=784 (pixels), apical=10 (teacher probs)\n",
        "        self.tc_lif = TwoCompartmentLIF(\n",
        "            in_basal=28*28,\n",
        "            in_apical=10,\n",
        "            hidden_size=hidden_size,\n",
        "            v_th=1.0,\n",
        "            apical_gain=0.5   # apical actually contributes\n",
        "        )\n",
        "        self.readout = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, images, use_teacher_context=True):\n",
        "        \"\"\"\n",
        "        images: [B,1,28,28]\n",
        "        \"\"\"\n",
        "        B = images.size(0)\n",
        "        device = images.device\n",
        "\n",
        "        # Basal: raw pixels\n",
        "        flat = images.view(B, -1)                 # [B,784]\n",
        "        xb_seq = flat.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,784]\n",
        "\n",
        "        # Apical: teacher belief over classes\n",
        "        if use_teacher_context:\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = self.teacher(images)            # [B,10]\n",
        "                teacher_ctx = F.softmax(teacher_logits, dim=-1)  # [B,10] (or keep logits)\n",
        "            xa_seq = teacher_ctx.unsqueeze(0).repeat(self.num_steps, 1, 1)  # [T,B,10]\n",
        "        else:\n",
        "            xa_seq = torch.zeros(self.num_steps, B, 10, device=device)\n",
        "\n",
        "        spikes, _ = self.tc_lif(xb_seq, xa_seq)  # [T,B,H]\n",
        "        spike_counts = spikes.sum(dim=0)         # [B,H]\n",
        "        logits = self.readout(spike_counts)      # [B,10]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "hrS4dPYN8lA8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_two_comp_with_teacher(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    num_epochs=5,\n",
        "    lr=1e-3,\n",
        "    use_context_train=True,\n",
        "    use_context_test=True,\n",
        "    device=device,\n",
        "):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_examples = 0.0, 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images, use_teacher_context=use_context_train)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_examples\n",
        "        train_acc  = total_correct / total_examples\n",
        "\n",
        "        # eval\n",
        "        model.eval()\n",
        "        tot_corr, tot_ex = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits = model(images, use_teacher_context=use_context_test)\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                tot_corr += (preds == labels).sum().item()\n",
        "                tot_ex   += labels.size(0)\n",
        "        test_acc = tot_corr / tot_ex\n",
        "\n",
        "        print(f\"Epoch {epoch:2d} | train loss {train_loss:.4f} | \"\n",
        "              f\"train acc {train_acc:.4f} | test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "AQC1rwMi8-RS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluating the teacher/student two compartment model"
      ],
      "metadata": {
        "id": "wvjYFa77PnwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the two-compartment SNN with teacher context\n",
        "ctx_model = TwoCompartmentMNISTNetWithTeacher(\n",
        "    teacher=teacher,   # frozen teacher\n",
        "    hidden_size=256,\n",
        "    num_steps=25\n",
        ").to(device)\n",
        "\n",
        "print(\"=== Train with teacher context ON, test with context ON ===\")\n",
        "train_two_comp_with_teacher(\n",
        "    ctx_model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    num_epochs=5,\n",
        "    lr=1e-3,\n",
        "    use_context_train=True,\n",
        "    use_context_test=True,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "#evaluate SAME model with context OFF vs ON\n",
        "def eval_with_context_flag(model, test_loader, use_context_test, device=device):\n",
        "    model.eval()\n",
        "    total_correct, total_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits = model(images, use_teacher_context=use_context_test)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_examples += labels.size(0)\n",
        "    return total_correct / total_examples\n",
        "\n",
        "acc_with_ctx = eval_with_context_flag(ctx_model, test_loader, use_context_test=True)\n",
        "acc_no_ctx   = eval_with_context_flag(ctx_model, test_loader, use_context_test=False)\n",
        "\n",
        "print(f\"Final test acc WITH teacher context : {acc_with_ctx:.4f}\")\n",
        "print(f\"Final test acc WITHOUT apical input : {acc_no_ctx:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzPe6OnW9Dsm",
        "outputId": "46bda781-7cbd-4b81-b49b-983b1e67b265"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train with teacher context ON, test with context ON ===\n",
            "Epoch  0 | train loss 0.3748 | train acc 0.9067 | test acc 0.9094\n",
            "Epoch  1 | train loss 0.2721 | train acc 0.9294 | test acc 0.9192\n",
            "Epoch  2 | train loss 0.2927 | train acc 0.9274 | test acc 0.9408\n",
            "Epoch  3 | train loss 0.2821 | train acc 0.9298 | test acc 0.9307\n",
            "Epoch  4 | train loss 0.2823 | train acc 0.9316 | test acc 0.9184\n",
            "Final test acc WITH teacher context : 0.9184\n",
            "Final test acc WITHOUT apical input : 0.8760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the model does better with the context. However, this is not suprising since the context signal is coming from a very strong model. Still this setup shows that strong context and help the SNN."
      ],
      "metadata": {
        "id": "pCnGR3G6PsT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall conclusions\n",
        "\n",
        "- Two-compartment spiking neurons **do** change how the network processes information:\n",
        "  - Basal and apical inputs are used in **non-redundant** ways.\n",
        "  - Context-like signals (e.g. teacher predictions) routed through apical dendrites can influence performance and robustness.\n",
        "\n",
        "- However, on MNIST:\n",
        "  - A two-compartment SNN **does not automatically beat** a well-trained ANN.\n",
        "  - This matches the intuition that **one biological feature** (compartments) is not enough on its own to recreate the full power of the brain.\n",
        "\n",
        "- In the brain, compartments interact with:\n",
        "  - Nonlinear dendritic integration,\n",
        "  - Inhibition and gating,\n",
        "  - Local learning rules,\n",
        "  - Multiple timescales and circuits.\n",
        "\n",
        "... ideas for future projects :)"
      ],
      "metadata": {
        "id": "zNqQ4RcpQ-uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bge3dxEBZk_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}